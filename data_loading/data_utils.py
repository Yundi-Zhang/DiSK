# In this script we define sampling strategies for the data. There are 4 different sampling strategies: random points sampling, random lines sampling, VISTA sampling, and radial sampling.

import numpy as np
from numpy.fft import fftshift, ifftshift, ifftn
from scipy.stats import multivariate_normal

from pathlib import Path
from typing import Union, Optional, Tuple, Callable
import nibabel as nib
import time
import os


class NormalLines_Sampler(object):
    """Undersampling strategy to generate a random mask with a given acceleration factor.
    The dimension of the mask is determined by the mask_dim parameter. The mask is generated by randomly
    selecting points in the data space based on normal distribution."""
    
    def __init__(self, acc, data_shape, seed=None, **kwargs):
        self.seed = seed
        self.acc = acc
        self.data_shape = data_shape
        self.mask_shape = self.data_shape
    
    def get_mask(self):
        Nx, Ny, N = self.mask_shape[0], self.mask_shape[1], np.prod(self.mask_shape[2:])
        lines_num = int(Ny * N / self.acc)
        mask = np.zeros((Nx, Ny, N), dtype=np.uint8)
        pos = np.arange(Ny)
        mean = Ny/2
        cov = Ny/0.3
        normal_pdf = normal_dist(pos, mean, cov)
        normal_p = normal_pdf / np.sum(normal_pdf)
        for i in range(N):
            nd_indices = np.random.choice(Ny, lines_num // N, replace=False, p=normal_p)
            mask[:, nd_indices, i] = 1
        rand_lines_num = lines_num - lines_num // N * N
        if rand_lines_num > 0:
            rand_lines_idx = np.random.choice(Ny * N, rand_lines_num, replace=False)
            mask_ = mask.reshape((Nx, Ny * N))
            mask_[:, rand_lines_idx] = 1
            mask = mask_.reshape(self.mask_shape)
        else:
            mask = mask.reshape(self.mask_shape)
        return mask


class SimulateCartesian(object):

    @staticmethod
    def _fspecial(shape, sigma):
        """
        2D gaussian mask - should give the same result as MATLAB's
        fspecial('gaussian',[shape],[sigma])
        https://stackoverflow.com/questions/17190649/how-to-obtain-a-gaussian-filter-in-python
        """
        m, n = [(ss-1.)/2. for ss in shape]
        y, x = np.ogrid[-m:m+1, -n:n+1]
        h = np.exp( -(x * x + y * y) / (2. * sigma * sigma) )
        h[ h < np.finfo(h.dtype).eps * h.max() ] = 0
        sumh = h.sum()
        if sumh != 0:
            h /= sumh
        return h
    
    @staticmethod
    def _transform_kspace_to_image(k, dim=None, img_shape=None):
        """ Computes the Fourier transform from k-space to image space
        along a given or all dimensions
        :param k: k-space data
        :param dim: vector of dimensions to transform
        :param img_shape: desired shape of output image
        :returns: data in image space (along transformed dimensions)
        """
        if not dim:
            dim = range(k.ndim)

        img = fftshift(ifftn(ifftshift(k, axes=dim), s=img_shape, axes=dim), axes=dim)
        img *= np.sqrt(np.prod(np.take(img.shape, dim)))
        return img
    
    def _addSynthethicPhase(self, img):
        """
        Add synthetic phase to real-valued image. The phase is sampled from gaussian B0 variations (similar to LORAKS).
        """
        b0 = np.random.normal(4, 0.5, (img.shape[-2], img.shape[-1]))
        smoother = self._fspecial((img.shape[-2], img.shape[-1]), max(1.0, round(img.shape[-2] / 200)))
        b00 = self._transform_kspace_to_image(b0 * smoother)
        img = img * np.exp(1j * np.angle(b00))
        return img
    
    def __call__(self, arg):
        img = self._addSynthethicPhase(arg).astype(np.complex64)
        img = np.ascontiguousarray(img)
        return img
    

def normal_dist(pos, mean, cov):
    """Generate normal distribution probability density function

    Args:
        pos (int or nd.array or torch.tensor): qurey values. When x is a vector, the function returns the probability density of each element in x. When x is a matrix, the function returns the probability density of each column of x.
        mean: mean of the distribution
        cov: covariance matrix of the distribution

    Returns:
        nd.array: probability density of the normal distribution
    """
    pdf = multivariate_normal.pdf(pos, mean=mean, cov=cov)
    return pdf


def get_biggest_2D_slice_bbox(segmentation: np.ndarray, padding: int = 10) \
        -> np.ndarray:  # [Y1, X1, Y2, X2]
    """ Collapse all non-height/width dimensions into a 2D mask using the max operation
     such that the largest slice/frame is taken into account.
     Then find max and min indices of foreground mask to create the tightest possible bbox.
     Add padding to bbox by taking into account image's edges"""
    if len(segmentation.shape) > 2:
        non_slice_dims = tuple(range(2, len(segmentation.shape)))
        segmentation = segmentation.max(axis=non_slice_dims)
    indices = np.argwhere(segmentation > 0)
    min_indices = indices.min(0) - padding
    min_indices = np.maximum(min_indices, 0)
    assert len(min_indices) == 2
    max_indices = indices.max(0) + padding
    max_indices = np.minimum(max_indices, np.array(segmentation.shape[:2]) - 1)
    assert len(max_indices) == 2
    bbox = np.concatenate((min_indices, max_indices), axis=0)
    return bbox


def find_sax_images(load_dir: Union[str, Path],
                    num_cases: int = -1,
                    case_start_idx: int = 0,
                    bbox_func: Optional[Callable] = get_biggest_2D_slice_bbox,
                    img_file_name="sa.nii.gz",
                    seg_file_name="seg_sa.nii.gz",
                    **kwargs):
    """
    :param load_dir: Data directory where subjects are layed out.
    :param num_cases: Number of subjects to extract before breaking out of the data gathering loop.
                      If value is negative, collect all subjects.
    :param case_start_idx: Index of subject at which to start. Used to avoid loading same subjects
                            into train/val/test datasets. Ex: If you want to use 1000 subjects for
                            training and 100 for validation -> Train_start_idx = 0, Val_start_idx = 1000
    :param bbox_func: Function to be used to extract bounding boxes.
                      Should follow format [Y1, X1, ..., Y2, X2, ...]
    :param img_file_name: Naming convention of the image files you are intending to extract.
    :param seg_file_name: Naming convention of the segmentation files you are intending to extract.
    """
    assert num_cases != 0
    ims = []
    segs = []
    bboxes = []
    count = 0
    start_time = time.time()
    spcs = []  # Used to collect dataset statistics if necessary
    shapes = []
    load_dir = Path(load_dir)
    # Used to collect dataset statistics if necessary
    for i, parent in enumerate(sorted(os.listdir(str(load_dir)))):
        if i < case_start_idx:
            continue                            # Skip all subjects not belonging to this dataset
        if count >= num_cases > 0:
            break                               # If we have collected enough subjects, break

        im_path = load_dir / parent / img_file_name
        seg_path = load_dir / parent / seg_file_name
        # Make sure both image and segmentation files exist
        if not os.path.exists(im_path):
            continue
        if not os.path.exists(seg_path):
            continue
        try:
            im = nib.load(im_path)
            if im.shape[2] < 9:
                print(f"Found an image with suspiciously low number of SAX slices: {im.shape[2]} slices. {im_path.parent.name}")
                continue
            if im.shape[3] != 50:
                print(f"Found an image with suspicious number of SAX frames: {im.shape[3]} frames. {im_path.parent.name}")
                continue
            if bbox_func is not None:
                # Get bounding box of where foreground mask is present
                seg = nib.load(seg_path).get_fdata()
                bbox = bbox_func(seg)
                if (bbox[2] - bbox[0]) / (bbox[3] - bbox[1]) < 0.9 or (bbox[3] - bbox[1]) / (bbox[2] - bbox[0]) < 0.9:
                    print(f"Bounding box was too far from being square. Side lengths:", (bbox[2] - bbox[0]), (bbox[3] - bbox[1]))
                    continue
                bboxes.append(bbox)

            ims.append(im_path)
            segs.append(seg_path)

            shapes.append(im.shape)             # Used to collect dataset statistics if necessary
            spcs.append(im.header.get_zooms())  # Used to collect dataset statistics if necessary
        except Exception as e:
            continue

        count += 1

    if num_cases > 0 and count != num_cases:
        raise ValueError(f"Did not find required amount of cases ({num_cases}) in directory: {load_dir}")

    elapsed = time.time() - start_time
    print(f"Found {count} cases in {elapsed//60}m {int(elapsed%60)}s.")
    print(f'The data searching range is from {case_start_idx} to {i}.')
    return ims, segs, bboxes if bbox_func is not None else None